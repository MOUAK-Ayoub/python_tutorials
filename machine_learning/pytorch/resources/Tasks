Choose a challenge from ens platform to begin with
Plot accuracy as function of the number of kernels used for convolution
Impact of regularisation on accuracy
Use of batch noramalization and dropout
KNN for CIFAR10 dataset classification
Use wavelets for cnn and fourier transform
Using spring or framework like spring for aop and DI, for autowiring different datasets
optimize the relations between the files
Logging and plotting practices in python
add a dictionary for dataset init  which take batch size and other parameters
Adding dataset validation during training
Determine what is learning show weight as images, and the output of covolution and the type of filters learned for cnn , say
Evolving the function plot_accuracy_by_criteria to determine dynamically the message
Reading design patterns
Implementing Multiprocessing for cpu and gpu in pytorch
Reverify that all operations executes on cuda when it exists
Pattern recognition and machine learning

-------------------------------------------------------------------------------------------------------------------
See adaline madalaine of course 2 and the sequel of slides of lecture 1
voir PCA et theorie de l information
ecrire le code de tensorflow des cnn d ibm utilisant numpy
-----------------------------------------------------------------------------------------
Tester dans watson studio
ajouter des donn√©es dans le dataset aleatoire,..
Ameliorer 231 neural net
tester l'impact des differents parametres reg, N, K,... voir les images sous forme video
ecrir une partie web pour l 'appl, forme web service ....
using svm to this example
try  to write a generic code, a class, for the net, take as an example pytorch
applying compressin techniques, from communication theory, to reuce the dimension
-----------------------------------------------------------------------------------------
Tester l'exemple de recitation 1 with finding explicit boundary,
and comparing if the nn will find the correct boundary
----------------------------------------------------------------------------------------------------
Lecture 5
find the number of iteration for the gradient descent algorithm find the solution
write the two the two algorithms the rosenblatt and GD ,
I can see the minimum by drawing the loss function , which is dependent on two parameters,
the loss befor adding and after adding the point
I think if we give to the point a large probabilty the GD can find the solution

-------------------------------------------------------------------------------------------------------------------
gradient descent with mini batch
mini batch normalisation
adding factors to parameters so different optimizers
changing the learning rate over epoch
using a threshold for the gradient explosion problem
dropout
the loss function is globally decreasing with respect to epoch , always??
why training using a large num of epochs will lead to overfitting

---------------------------------------------------------------------------------------------------------------
Reorganise the code , with classes,...;
-----------------------------------------------------------------------------------------------------------------------
for gradient descent formulas see 22m36
understanding what happened with the two examples in CS231
why MLP works on the two examples of CS231





